{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run grid search on classifiers and their possible parameters and then return the best result\n",
    "def grid_search_classifiers(X_train, X_test, y_train, y_test, clfs, clf_to_gs_params_dict, cv=5):\n",
    "    reports = []\n",
    "    f_score_avgs = []\n",
    "    gs_list = []\n",
    "    for clf in clfs:\n",
    "        print(f\"\\tTraining classifier: {clf.__class__}...\")\n",
    "        gs = GridSearchCV(clf, clf_to_gs_params_dict[clf.__class__], cv=cv)\n",
    "        gs.fit(X_train, y_train)\n",
    "        gs_list.append(gs)\n",
    "        labels = np.unique(y_train)\n",
    "        report = util.test_classifier(gs, X_test, y_test, labels)\n",
    "        reports.append(report)\n",
    "        f_score_avg = np.mean(report[\"f_score\"])\n",
    "        f_score_avgs.append(f_score_avg)\n",
    "        \n",
    "    results_sorted = sorted(zip(gs_list, f_score_avgs, reports), key=lambda x: x[1], reverse=True)\n",
    "    best_result = results_sorted[0]\n",
    "            \n",
    "    return best_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [\n",
    "    GaussianNB(),\n",
    "    LogisticRegression(),\n",
    "    KNeighborsClassifier(),\n",
    "    MLPClassifier()\n",
    "]\n",
    "\n",
    "clf_to_gs_params_dict = {\n",
    "    GaussianNB().__class__: {\n",
    "    },\n",
    "    LogisticRegression().__class__: {\n",
    "        \"penalty\": (\"l1\", \"l2\"),\n",
    "        \"C\": (1.0e0, 1.0e-1, 1.0e-2, 1.0e-3, 1.0e-4, 1.0e-5)\n",
    "    },\n",
    "    KNeighborsClassifier().__class__: {\n",
    "        \"n_neighbors\": (4, 8, 12, 16)\n",
    "    },\n",
    "    MLPClassifier().__class__: {\n",
    "        \"hidden_layer_sizes\": ((100,), (100, 50), (100, 50, 20)),\n",
    "        \"activation\": (\"relu\",),\n",
    "        \"alpha\": (1.0e-1, 1.0e-2, 1.0e-3),\n",
    "        \"batch_size\": (8, 16, 64),\n",
    "        \"learning_rate\": (\"constant\", \"adaptive\"),\n",
    "        \"learning_rate_init\": (1.0e-2, 1.0e-3, 1.0e-4),\n",
    "        \"early_stopping\": (True,),\n",
    "        \"momentum\": (0.5, 0.9)\n",
    "    }\n",
    "}\n",
    "\n",
    "# split X into train and test sets using established indices\n",
    "X = np.array([model.docvecs[i] for i in range(len(model.docvecs))])\n",
    "X_train, X_test = split_X_into_train_test_sets(X, train_indices, test_indices)\n",
    "\n",
    "best_gs, best_f_score_avg, best_report = grid_search_classifiers(X_train, X_test, y_train, y_test, clfs, clf_to_gs_params_dict)\n",
    "print(f\"\\tBest classifier: {best_gs.best_estimator_}\")\n",
    "print(f\"\\tAverage F-score: {best_f_score_avg}\")\n",
    "print(f\"\\tBest hyperparameters: {best_gs.best_params_}\")\n",
    "print(\"\\tClassification report:\\n\")\n",
    "print(best_report)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(util.CLASSIFIER_MODEL_FILE, \"wb\") as f: \n",
    "    pickle.dump(best_gs.best_estimator_, f)\n",
    "    \n",
    "with open(util.CLASSIFIER_HYPERPARAMETERS_FILE, \"wb\") as f:\n",
    "    pickle.dump(best_gs.best_params_, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = best_gs.best_estimator_.__class__.__name__\n",
    "hyperparams = best_gs.best_params_\n",
    "\n",
    "if len(hyperparams) > 0:\n",
    "    clf_untrained = globals()[classifier_name](hyperparams)\n",
    "else:\n",
    "    clf_untrained = globals()[classifier_name]()\n",
    "\n",
    "results = util.train_and_test_classifier_k_fold(X, y, clf_untrained, k_fold=10)\n",
    "for i, (report, f_score_avg) in enumerate(results):\n",
    "    print(f\"k: {i+1}/{len(results)}\")\n",
    "    print(f\"F-Score Average: {f_score_avg}\")\n",
    "    print(report)\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
